% !TeX spellcheck = pl_PL
\documentclass[16]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\usepackage[polish]{babel}
\usepackage{polski}
\usepackage{titlesec}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{float}
\usepackage{mathtools}
\usepackage{array}
\newenvironment{steps}[1]{\begin{enumerate}[label=#1 \arabic*]}{\end{enumerate}}

\setlength{\parindent}{0pt}
\begin{document}
	\begin{titlepage}
		\begin{center}
			\vspace*{0cm}
			\textsc{\LARGE \bfseries Politechnika Warszawska}\\[1.5cm]
			\textsc{\large Wydział Elektroniki i Technik Informacyjnych}\\[0.2cm]
			\textsc{\large Informatyka}\\[1.6cm]
			\textsc{\LARGE Metody Odkrywania Wiedzy }\\[1.4cm]
			\rule{\linewidth}{0.5mm} \\ [0.8cm]
      \textsc{\Huge Klasyfikator bayesowski} \\ [0.4cm]
			\textsc{\Huge typu AODE} \\ [0.8cm]
			\textsc{\large Nie-całkiem-naiwny klasyfikator bayesowski typu AODE \\ (averaged one-dependence estimators). \\ Porównania ze standardowym naiwnym klasyfikatorem bayesowskim i innymi algorytmami klasyfikacji dostępnymi w R.} \\ [0.8cm]
			\textsc{\LARGE{\underline {Sprawozdanie z etapu II}}} \\ [0.8cm]
			\rule{\linewidth}{0.5mm} \\ [1cm]
			
			\begin{flushright}
				Wykonali: \\[0.2cm]
				{\large Paweł Guz}\\[0.2cm]
				{\large Mateusz Kędrzyński}\\[0.8cm]
				Prowadzący: \\[0.2cm]
				{\large dr inż. Paweł Cichosz} \\ [0.8cm]
			\end{flushright}
			\vfill
			{\large Warszawa, 10 IV 2015}
		\end{center}
	\end{titlepage}
	
\section{Opis implementacji}

% Paweł, dużo łatwiej będzie Ci to napisać, jak możesz to uzupełnij :-)
% CHyba nie ma się też co rozwodzić za bardzo 

\section{Testy poprawności implementacji}
W celu udowodnienia poprawności implementacji algorytmu AODE użyto następującego zbioru danych ( plik: \textit{weather.csv}):

\begin{tabular}{ |l|l|l|l|l| }
\hline
outlook & temperature & humidity & wind & play
\\ \hline
sunny & hot & high & normal & no
\\ \hline
sunny & hot & high & high & no
\\ \hline
overcast & hot & high & normal & yes
\\ \hline
rainy & mild & high & normal & yes
\\ \hline
rainy & cold & normal & normal & yes
\\ \hline
rainy & cold & normal & high & no
\\ \hline
overcast & cold & normal & high & yes
\\ \hline
sunny & mild & high & normal & no
\\ \hline
sunny & cold & normal & normal & yes
\\ \hline
rainy & mild & normal & normal & yes
\\ \hline
sunny & mild & normal & high & yes
\\ \hline
overcast & mild & high & high & yes
\\ \hline
overcast & hot & normal & normal & yes
\\ \hline
rainy & mild & high & high & no
\\ \hline
\end{tabular}\\

Zbudowano model i dla każdego rekordu z danych trenujących wyznaczono klasę decyzyjną. Wyniki były w pełni zgodne z danymi wejściowymi. 



\section{Testy i porównania z innymi algorytmami}

\subsection{Weather}

\subsubsection{Charakterystyka zbioru}

\begin{itemize}
	\item Liczba atrybutów: 4
	\item Liczba klas: 2
	\item Liczba przykładów: 14
	\item Podział: modele były oceniane oraz testowane na tym samym zbiorze danych
\end{itemize}

\subsubsection{Wyniki}

\begin{tabular}{ |l|l|l|l|l|l|l| }
\hline
&  Accuracy & Error & Recall/Sensivity & Precision & Specifity & FMeasure
\\ \hline
AODE ( $m < 5$)& 1 & 0 & 1 & 1 & 1 & 1
\\ \hline
AODE ( $m = 5, 6$) & 0.93 & 0.07 & 1 & 0.9 & 0.8 & 0.95
\\ \hline
Naive Bayes & 0.93 & 0.07 & 1 & 0.9 & 0.8 & 0.95
\\ \hline
C4.5 & 1 & 0 & 1 & 1 & 1 & 1
\\ \hline
TAN & 0.79 & 0.21 & 0.67 & 1 & 1 & 0.8
\\ \hline
LBR & 0.93 & 0.07 & 0.9 & 1 & 1 & 0.95
\\ \hline

\end{tabular}
\\\\
Dla prostego zbioru danych jakim jest zbiór "weather", wyniki uzyskane przez algorytm AODE dają bardzo dobry rezultat, lepszy niż naiwny bayes. Parametr \textit{m} jeżeli jest mniejszy niż 5 nie ma wpływu na wyniki. Jest to spowodowane tym że każda wartość każdego atrybutu występuje w co najmniej 4 przykładach. Dla parametru $m = 5$ następuje zwiększenie ilości błędów. Zależność ta jest spowodowana małą ilością przykładów - każdy przykład jest rozróżnialny. 

\subsection{Cars}
\subsubsection{Charakterystyka zbioru}
\begin{itemize}
	\item Liczba atrybutów: 6
	\item Liczba klas: 4
	\item Liczba przykładów: 1728
	\item Dane uczące: 864 przykładów (50\% wszystkich przykładów wybranych losowo) 
	\item Dane testowe: 864 przykładów (pozostałe przykłady)
\end{itemize}
\subsubsection{Wyniki}
\begin{tabular}{ |l|l|l| }
\hline
	 &  Accuracy & Error
 \\ \hline
 AODE ($m < 205$) & 0.911 & 0.089
 \\ \hline
 AODE ($m = 205$) &  0.913 & 0.087
 \\ \hline
 AODE ($m = 210$) & 0.907 & 0.093
 \\ \hline
 AODE ($m = 250$) & 0.894 & 0.106
 \\ \hline
 AODE ($m = 280$) & 0.889 & 0.111
 \\ \hline
  Naive Bayes & 0.866 & 0.134
 \\ \hline
 TAN & 0.3 & 0.7
 \\ \hline
 LBR & 0.932 & 0.068
 \\ \hline
 C4.5 & 0.896 & 0.104
 \\ \hline
\end{tabular}\\

Wpływ parametru m na wynik jest zależny od rozkładu wartości poszczególnych atrybutów. W zbiorze danych \textit{car} liczba wystąpień wartości atrybutów zawiera się w przedziale [205, 305] dlatego też atrybut m powinien być mniejszy niż górna granica tego przedziału. Widać tutaj pierwszy z problemów w doborze parametru \textit{m} - zależy od ilości danych uczących. W przypadku inkrementalnego tworzenia modelu parametr ten wraz z douczaniem modelu, powinien być zmieniany. 


\subsection{Mushrums}
\subsubsection{Charakterystyka zbioru}
\begin{itemize}
	
	\item {\LARGE DO UZUPEŁNIENIA }
	
	\item Dane uczące: 50\% wszystkich przykładów wybranych losowo 
	\item Dane testowe: pozostałe przykłady
\end{itemize}

\subsubsection{Wyniki}

\begin{tabular}{ |l|l|l| }
\hline
&  Accuracy & Error
\\ \hline
AODE ($m = 0$) & 0.666 & 0.334
\\ \hline
AODE ($m = 300$) & 0.668 & 0.332
\\ \hline
AODE ($m = 600$) & 0.665 & 0.335
\\ \hline
AODE ($m = 1500$) & 0.662 & 0.338
\\ \hline
AODE ($m = 2200$) & 0.638 & 0.362
\\ \hline
AODE ($m = 2800$) & 0.639 & 0.361
\\ \hline
AODE ($m = 3300$) & 0.638 & 0.362
\\ \hline
AODE ($m = 3500$) &0.637 & 0.363
\\ \hline
Naive Bayes & 0.643 & 0.357
\\ \hline
TAN & 0.144 & 0.856
\\ \hline
LBR & 0.669 & 0.331
\\ \hline
C4.5 & 0.616 & 0.384
\\ \hline
\end{tabular} \\\\

Ilości atrybutów od 18 do 3973. 

im większy parametr M tym szybciej się liczy

\section{Wnioski}


\begin{thebibliography}{9}
	
	\bibitem{lamport94}
		Paweł Cichosz,
		\emph{Materiały do wykładu z MOW}
			
	\bibitem{lamport94}
		Geoffrey I. Webb
		Janice R. Boughton
		Zhihai Wang,
		\emph{Not so naive Bayes: Aggregating one-dependence estimators}.
		School of Computer Science and Software Engineering
    
  \bibitem{lamport94}
		Paweł Cichosz,
		\emph{Data Mining Algorithms: Explained Using R}
\end{thebibliography}

\end{document}
